{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "microsoft": {}
      },
      "outputs": [],
      "source": [
        "account =  # your storage account name\n",
        "container =  # your container name\n",
        "\n",
        "bronze = f\"abfss://{container}@{account}.dfs.core.windows.net/bronze\"\n",
        "silver = f\"abfss://{container}@{account}.dfs.core.windows.net/silver\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "orders_bronze = f\"{bronze}/orders\"\n",
        "fact_orders_silver = f\"{silver}/fact_orders\"\n",
        "\n",
        "ob = spark.read.parquet(orders_bronze)\n",
        "\n",
        "fo = (ob\n",
        "    # normalize column names already done in bronze; cast types here\n",
        "    .withColumn(\"order_id\", F.col(\"order_id\").cast(\"int\"))\n",
        "    .withColumn(\"user_id\", F.col(\"user_id\").cast(\"int\"))\n",
        "    .withColumn(\"eval_set\", F.col(\"eval_set\").cast(\"string\"))\n",
        "    .withColumn(\"order_number\", F.col(\"order_number\").cast(\"int\"))\n",
        "    .withColumn(\"order_dow\", F.col(\"order_dow\").cast(\"int\"))\n",
        "    # sample showed \"08\" / \"07\" — cast to int safely\n",
        "    .withColumn(\"order_hour_of_day\", F.col(\"order_hour_of_day\").cast(\"int\"))\n",
        "    .withColumn(\"days_since_prior_order\", F.col(\"days_since_prior_order\").cast(\"double\"))\n",
        "    .select(\"order_id\",\"user_id\",\"eval_set\",\"order_number\",\"order_dow\",\n",
        "            \"order_hour_of_day\",\"days_since_prior_order\")\n",
        ")\n",
        "\n",
        "# write silver\n",
        "(fo.repartition(1)\n",
        "   .write.mode(\"overwrite\")\n",
        "   .parquet(fact_orders_silver))\n",
        "\n",
        "print(\"silver.fact_orders:\", fact_orders_silver, \"rows:\", fo.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "products_bronze    = f\"{bronze}/products\"\n",
        "aisles_bronze      = f\"{bronze}/aisles\"\n",
        "departments_bronze = f\"{bronze}/departments\"\n",
        "dim_products_silver = f\"{silver}/dim_products\"\n",
        "\n",
        "p  = spark.read.parquet(products_bronze)\n",
        "a  = spark.read.parquet(aisles_bronze)\n",
        "d  = spark.read.parquet(departments_bronze)\n",
        "\n",
        "# cast + join\n",
        "p = (p\n",
        "     .withColumn(\"product_id\", F.col(\"product_id\").cast(\"int\"))\n",
        "     .withColumn(\"product_name\", F.col(\"product_name\").cast(\"string\"))\n",
        "     .withColumn(\"aisle_id\", F.col(\"aisle_id\").cast(\"int\"))\n",
        "     .withColumn(\"department_id\", F.col(\"department_id\").cast(\"int\"))\n",
        ")\n",
        "\n",
        "a = (a\n",
        "     .withColumn(\"aisle_id\", F.col(\"aisle_id\").cast(\"int\"))\n",
        "     .withColumnRenamed(\"aisle\", \"aisle_name\")\n",
        ")\n",
        "\n",
        "d = (d\n",
        "     .withColumn(\"department_id\", F.col(\"department_id\").cast(\"int\"))\n",
        "     .withColumnRenamed(\"department\", \"department_name\")\n",
        ")\n",
        "\n",
        "dim_products = (p\n",
        "    .join(a, \"aisle_id\", \"left\")\n",
        "    .join(d, \"department_id\", \"left\")\n",
        "    .select(\"product_id\",\"product_name\",\"aisle_id\",\"aisle_name\",\"department_id\",\"department_name\")\n",
        ")\n",
        "\n",
        "(dim_products.repartition(1)\n",
        "    .write.mode(\"overwrite\")\n",
        "    .parquet(dim_products_silver))\n",
        "\n",
        "print(\" silver.dim_products:\", dim_products_silver, \"rows:\", dim_products.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "opp_bronze = f\"{bronze}/order_products__prior\"\n",
        "opt_bronze = f\"{bronze}/order_products__train\"\n",
        "fact_op_silver = f\"{silver}/fact_order_products\"\n",
        "\n",
        "prior = (spark.read.parquet(opp_bronze)\n",
        "         .withColumn(\"split_source\", F.lit(\"prior\")))\n",
        "\n",
        "train = (spark.read.parquet(opt_bronze)\n",
        "         .withColumn(\"split_source\", F.lit(\"train\")))\n",
        "\n",
        "# ensure consistent types\n",
        "def cast_op(df):\n",
        "    return (df\n",
        "        .withColumn(\"order_id\", F.col(\"order_id\").cast(\"int\"))\n",
        "        .withColumn(\"product_id\", F.col(\"product_id\").cast(\"int\"))\n",
        "        .withColumn(\"add_to_cart_order\", F.col(\"add_to_cart_order\").cast(\"int\"))\n",
        "        .withColumn(\"reordered\", F.col(\"reordered\").cast(\"int\"))\n",
        "        .select(\"order_id\",\"product_id\",\"add_to_cart_order\",\"reordered\",\"split_source\")\n",
        "    )\n",
        "\n",
        "fact_op = cast_op(prior).unionByName(cast_op(train))\n",
        "\n",
        "(fact_op.repartition(8)  # a few files for parallel reads later\n",
        "   .write.mode(\"overwrite\")\n",
        "   .parquet(fact_op_silver))\n",
        "\n",
        "print(\" silver.fact_order_products:\", fact_op_silver, \"rows:\", fact_op.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fact_orders_silver = f\"{silver}/fact_orders\"\n",
        "fact_op_silver     = f\"{silver}/fact_order_products\"\n",
        "dim_users_silver   = f\"{silver}/dim_users\"\n",
        "fup_silver         = f\"{silver}/fact_user_product\"\n",
        "\n",
        "fo = spark.read.parquet(fact_orders_silver)\n",
        "op = spark.read.parquet(fact_op_silver)\n",
        "\n",
        "# basket sizes per order\n",
        "basket_sizes = (op.groupBy(\"order_id\").agg(F.count(\"*\").alias(\"basket_size\")))\n",
        "\n",
        "# join to orders to compute user metrics\n",
        "orders_plus = (fo.join(basket_sizes, \"order_id\", \"left\"))\n",
        "\n",
        "# user reorder ratio: needs reordered flag joined at user level\n",
        "user_reorders = (op.groupBy().count())  # just placeholder to avoid confusion\n",
        "\n",
        "# dim_users: user-level rollups\n",
        "dim_users = (orders_plus\n",
        "  .groupBy(\"user_id\")\n",
        "  .agg(\n",
        "      F.countDistinct(\"order_id\").alias(\"total_orders\"),\n",
        "      F.avg(\"days_since_prior_order\").alias(\"avg_days_between_orders\"),\n",
        "      F.expr(\"percentile_approx(order_hour_of_day, 0.5)\").alias(\"median_order_hour\"),\n",
        "      F.expr(\"percentile_approx(order_dow, 0.5)\").alias(\"median_order_dow\"),\n",
        "      F.avg(\"basket_size\").alias(\"avg_basket_size\")\n",
        "  )\n",
        ")\n",
        "\n",
        "# compute user reorder ratio (share of line items with reordered=1)\n",
        "user_reorder_ratio = (op.groupBy(\"order_id\")\n",
        "                        .agg(F.avg(F.col(\"reordered\").cast(\"double\")).alias(\"order_reorder_rate\"))\n",
        "                      .join(fo.select(\"order_id\",\"user_id\"), \"order_id\", \"left\")\n",
        "                      .groupBy(\"user_id\")\n",
        "                      .agg(F.avg(\"order_reorder_rate\").alias(\"reorder_ratio\"))\n",
        "                     )\n",
        "\n",
        "dim_users = (dim_users.join(user_reorder_ratio, \"user_id\", \"left\")\n",
        "                      .fillna({\"reorder_ratio\": 0.0})\n",
        ")\n",
        "\n",
        "(dim_users.repartition(1)\n",
        "   .write.mode(\"overwrite\")\n",
        "   .parquet(dim_users_silver))\n",
        "\n",
        "print(\"silver.dim_users:\", dim_users_silver, \"rows:\", dim_users.count())\n",
        "\n",
        "# (optional) user × product history for ML features and analytics\n",
        "fup = (op.join(fo.select(\"order_id\",\"user_id\"), \"order_id\", \"left\")\n",
        "         .groupBy(\"user_id\",\"product_id\")\n",
        "         .agg(\n",
        "             F.count(\"*\").alias(\"times_purchased\"),\n",
        "             F.avg(\"add_to_cart_order\").alias(\"avg_add_to_cart_position\"),\n",
        "             F.max(\"reordered\").alias(\"ever_reordered\")\n",
        "         )\n",
        "      )\n",
        "\n",
        "(fup.repartition(8)\n",
        "    .write.mode(\"overwrite\")\n",
        "    .parquet(fup_silver))\n",
        "\n",
        "print(\"silver.fact_user_product:\", fup_silver, \"rows:\", fup.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
