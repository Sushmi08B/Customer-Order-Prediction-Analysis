{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "microsoft": {}
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "import re\n",
        "\n",
        "\n",
        "account =  # your storage account name\n",
        "container =  # your container name\n",
        "date_folder =  # the landing_date where your 6 CSVs live\n",
        "\n",
        "raw_root    = f\"abfss://{container}@{account}.dfs.core.windows.net/raw/online_retail_ii/landing_date={date_folder}\"\n",
        "bronze_root = f\"abfss://{container}@{account}.dfs.core.windows.net/bronze\"\n",
        "\n",
        "\n",
        "# file base names (without .csv) â†’ bronze subfolder name\n",
        "tables = {\n",
        "\n",
        "    \"order_products__prior\": \"order_products__prior\",\n",
        "    \"order_products__train\": \"order_products__train\",\n",
        "    \"products\": \"products\",\n",
        "}\n",
        "\n",
        "def tidy_columns(df: DataFrame) -> DataFrame:\n",
        "    new = df\n",
        "    for c in df.columns:\n",
        "        nc = re.sub(r\"\\s+\", \"_\", c.strip().lower())\n",
        "        new = new.withColumnRenamed(c, nc)\n",
        "    return new\n",
        "\n",
        "def bronze_write(csv_name: str, bronze_name: str):\n",
        "    src = f\"{raw_root}/{csv_name}.csv\"\n",
        "    out = f\"{bronze_root}/{bronze_name}\"\n",
        "\n",
        "    df = (spark.read\n",
        "          .option(\"header\", True)\n",
        "          .option(\"inferSchema\", True)\n",
        "          .option(\"encoding\", \"UTF-8\")\n",
        "          .csv(src))\n",
        "\n",
        "    df = tidy_columns(df) \\\n",
        "         .withColumn(\"source_path\", F.input_file_name()) \\\n",
        "         .withColumn(\"ingest_ts\", F.current_timestamp()) \\\n",
        "         .withColumn(\"landing_date\", F.lit(date_folder))\n",
        "\n",
        "    # Write partitioned by landing_date for easy multi-day loads\n",
        "    (df.repartition(1)\n",
        "       .write.mode(\"overwrite\")\n",
        "       .partitionBy(\"landing_date\")\n",
        "       .parquet(out))\n",
        "\n",
        "    cnt = df.count()\n",
        "    print(f\"âœ… {csv_name}.csv â†’ {out}  (rows: {cnt})\")\n",
        "\n",
        "for csv_name, bronze_name in tables.items():\n",
        "    bronze_write(csv_name, bronze_name)\n",
        "\n",
        "print(\"ðŸŽ‰ Bronze complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
