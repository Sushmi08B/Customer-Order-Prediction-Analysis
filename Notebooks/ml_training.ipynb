{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "microsoft": {}
      },
      "outputs": [],
      "source": [
        "account =  # your storage account name\n",
        "container =  # your container name\n",
        "\n",
        "bronze = f\"abfss://{container}@{account}.dfs.core.windows.net/bronze\"\n",
        "silver = f\"abfss://{container}@{account}.dfs.core.windows.net/silver\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "orders_bronze = f\"{bronze}/orders\"\n",
        "fact_orders_silver = f\"{silver}/fact_orders\"\n",
        "\n",
        "ob = spark.read.parquet(orders_bronze)\n",
        "\n",
        "fo = (ob\n",
        "    # normalize column names already done in bronze; cast types here\n",
        "    .withColumn(\"order_id\", F.col(\"order_id\").cast(\"int\"))\n",
        "    .withColumn(\"user_id\", F.col(\"user_id\").cast(\"int\"))\n",
        "    .withColumn(\"eval_set\", F.col(\"eval_set\").cast(\"string\"))\n",
        "    .withColumn(\"order_number\", F.col(\"order_number\").cast(\"int\"))\n",
        "    .withColumn(\"order_dow\", F.col(\"order_dow\").cast(\"int\"))\n",
        "    # sample showed \"08\" / \"07\" — cast to int safely\n",
        "    .withColumn(\"order_hour_of_day\", F.col(\"order_hour_of_day\").cast(\"int\"))\n",
        "    .withColumn(\"days_since_prior_order\", F.col(\"days_since_prior_order\").cast(\"double\"))\n",
        "    .select(\"order_id\",\"user_id\",\"eval_set\",\"order_number\",\"order_dow\",\n",
        "            \"order_hour_of_day\",\"days_since_prior_order\")\n",
        ")\n",
        "\n",
        "# write silver\n",
        "(fo.repartition(1)\n",
        "   .write.mode(\"overwrite\")\n",
        "   .parquet(fact_orders_silver))\n",
        "\n",
        "print(\"silver.fact_orders:\", fact_orders_silver, \"rows:\", fo.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "products_bronze    = f\"{bronze}/products\"\n",
        "aisles_bronze      = f\"{bronze}/aisles\"\n",
        "departments_bronze = f\"{bronze}/departments\"\n",
        "dim_products_silver = f\"{silver}/dim_products\"\n",
        "\n",
        "p  = spark.read.parquet(products_bronze)\n",
        "a  = spark.read.parquet(aisles_bronze)\n",
        "d  = spark.read.parquet(departments_bronze)\n",
        "\n",
        "# cast + join\n",
        "p = (p\n",
        "     .withColumn(\"product_id\", F.col(\"product_id\").cast(\"int\"))\n",
        "     .withColumn(\"product_name\", F.col(\"product_name\").cast(\"string\"))\n",
        "     .withColumn(\"aisle_id\", F.col(\"aisle_id\").cast(\"int\"))\n",
        "     .withColumn(\"department_id\", F.col(\"department_id\").cast(\"int\"))\n",
        ")\n",
        "\n",
        "a = (a\n",
        "     .withColumn(\"aisle_id\", F.col(\"aisle_id\").cast(\"int\"))\n",
        "     .withColumnRenamed(\"aisle\", \"aisle_name\")\n",
        ")\n",
        "\n",
        "d = (d\n",
        "     .withColumn(\"department_id\", F.col(\"department_id\").cast(\"int\"))\n",
        "     .withColumnRenamed(\"department\", \"department_name\")\n",
        ")\n",
        "\n",
        "dim_products = (p\n",
        "    .join(a, \"aisle_id\", \"left\")\n",
        "    .join(d, \"department_id\", \"left\")\n",
        "    .select(\"product_id\",\"product_name\",\"aisle_id\",\"aisle_name\",\"department_id\",\"department_name\")\n",
        ")\n",
        "\n",
        "(dim_products.repartition(1)\n",
        "    .write.mode(\"overwrite\")\n",
        "    .parquet(dim_products_silver))\n",
        "\n",
        "print(\" silver.dim_products:\", dim_products_silver, \"rows:\", dim_products.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "opp_bronze = f\"{bronze}/order_products__prior\"\n",
        "opt_bronze = f\"{bronze}/order_products__train\"\n",
        "fact_op_silver = f\"{silver}/fact_order_products\"\n",
        "\n",
        "prior = (spark.read.parquet(opp_bronze)\n",
        "         .withColumn(\"split_source\", F.lit(\"prior\")))\n",
        "\n",
        "train = (spark.read.parquet(opt_bronze)\n",
        "         .withColumn(\"split_source\", F.lit(\"train\")))\n",
        "\n",
        "# ensure consistent types\n",
        "def cast_op(df):\n",
        "    return (df\n",
        "        .withColumn(\"order_id\", F.col(\"order_id\").cast(\"int\"))\n",
        "        .withColumn(\"product_id\", F.col(\"product_id\").cast(\"int\"))\n",
        "        .withColumn(\"add_to_cart_order\", F.col(\"add_to_cart_order\").cast(\"int\"))\n",
        "        .withColumn(\"reordered\", F.col(\"reordered\").cast(\"int\"))\n",
        "        .select(\"order_id\",\"product_id\",\"add_to_cart_order\",\"reordered\",\"split_source\")\n",
        "    )\n",
        "\n",
        "fact_op = cast_op(prior).unionByName(cast_op(train))\n",
        "\n",
        "(fact_op.repartition(8)  # a few files for parallel reads later\n",
        "   .write.mode(\"overwrite\")\n",
        "   .parquet(fact_op_silver))\n",
        "\n",
        "print(\" silver.fact_order_products:\", fact_op_silver, \"rows:\", fact_op.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fact_orders_silver = f\"{silver}/fact_orders\"\n",
        "fact_op_silver     = f\"{silver}/fact_order_products\"\n",
        "dim_users_silver   = f\"{silver}/dim_users\"\n",
        "fup_silver         = f\"{silver}/fact_user_product\"\n",
        "\n",
        "fo = spark.read.parquet(fact_orders_silver)\n",
        "op = spark.read.parquet(fact_op_silver)\n",
        "\n",
        "# basket sizes per order\n",
        "basket_sizes = (op.groupBy(\"order_id\").agg(F.count(\"*\").alias(\"basket_size\")))\n",
        "\n",
        "# join to orders to compute user metrics\n",
        "orders_plus = (fo.join(basket_sizes, \"order_id\", \"left\"))\n",
        "\n",
        "# user reorder ratio: needs reordered flag joined at user level\n",
        "user_reorders = (op.groupBy().count())  # just placeholder to avoid confusion\n",
        "\n",
        "# dim_users: user-level rollups\n",
        "dim_users = (orders_plus\n",
        "  .groupBy(\"user_id\")\n",
        "  .agg(\n",
        "      F.countDistinct(\"order_id\").alias(\"total_orders\"),\n",
        "      F.avg(\"days_since_prior_order\").alias(\"avg_days_between_orders\"),\n",
        "      F.expr(\"percentile_approx(order_hour_of_day, 0.5)\").alias(\"median_order_hour\"),\n",
        "      F.expr(\"percentile_approx(order_dow, 0.5)\").alias(\"median_order_dow\"),\n",
        "      F.avg(\"basket_size\").alias(\"avg_basket_size\")\n",
        "  )\n",
        ")\n",
        "\n",
        "# compute user reorder ratio (share of line items with reordered=1)\n",
        "user_reorder_ratio = (op.groupBy(\"order_id\")\n",
        "                        .agg(F.avg(F.col(\"reordered\").cast(\"double\")).alias(\"order_reorder_rate\"))\n",
        "                      .join(fo.select(\"order_id\",\"user_id\"), \"order_id\", \"left\")\n",
        "                      .groupBy(\"user_id\")\n",
        "                      .agg(F.avg(\"order_reorder_rate\").alias(\"reorder_ratio\"))\n",
        "                     )\n",
        "\n",
        "dim_users = (dim_users.join(user_reorder_ratio, \"user_id\", \"left\")\n",
        "                      .fillna({\"reorder_ratio\": 0.0})\n",
        ")\n",
        "\n",
        "(dim_users.repartition(1)\n",
        "   .write.mode(\"overwrite\")\n",
        "   .parquet(dim_users_silver))\n",
        "\n",
        "print(\"silver.dim_users:\", dim_users_silver, \"rows:\", dim_users.count())\n",
        "\n",
        "# (optional) user × product history for ML features and analytics\n",
        "fup = (op.join(fo.select(\"order_id\",\"user_id\"), \"order_id\", \"left\")\n",
        "         .groupBy(\"user_id\",\"product_id\")\n",
        "         .agg(\n",
        "             F.count(\"*\").alias(\"times_purchased\"),\n",
        "             F.avg(\"add_to_cart_order\").alias(\"avg_add_to_cart_position\"),\n",
        "             F.max(\"reordered\").alias(\"ever_reordered\")\n",
        "         )\n",
        "      )\n",
        "\n",
        "(fup.repartition(8)\n",
        "    .write.mode(\"overwrite\")\n",
        "    .parquet(fup_silver))\n",
        "\n",
        "print(\"silver.fact_user_product:\", fup_silver, \"rows:\", fup.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "account =  # your storage account name\n",
        "container =  # your container name\n",
        "silver = f\"abfss://{container}@{account}.dfs.core.windows.net/silver\"\n",
        "gold   = f\"abfss://{container}@{account}.dfs.core.windows.net/gold\"\n",
        "\n",
        "op = spark.read.parquet(f\"{silver}/fact_order_products\")\n",
        "\n",
        "# product popularity + reorder ratio\n",
        "prod_feats = (op.groupBy(\"product_id\")\n",
        "                .agg(\n",
        "                    F.count(\"*\").alias(\"prod_times_ordered\"),\n",
        "                    F.avg(F.col(\"reordered\").cast(\"double\")).alias(\"prod_reorder_ratio\")\n",
        "                ))\n",
        "\n",
        "(prod_feats.repartition(1)\n",
        "    .write.mode(\"overwrite\")\n",
        "    .parquet(f\"{gold}/product_features\"))\n",
        "\n",
        "print(\"gold.product_features written\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-08-25T00:42:10.1924143Z",
              "execution_start_time": "2025-08-25T00:41:16.5900832Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "60330b30-dfec-43c4-bf8a-b160ceda160d",
              "queued_time": "2025-08-25T00:41:16.5889101Z",
              "session_id": "5",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparksmall",
              "state": "finished",
              "statement_id": 3,
              "statement_ids": [
                3
              ]
            },
            "text/plain": [
              "StatementMeta(sparksmall, 5, 3, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rows: 33819106\n",
            "+-------+----------+---------+------------+-----------------------+------------------+-------------------+------------------+-------------------+---------------+------------------------+--------------+\n",
            "|user_id|product_id|reordered|total_orders|avg_days_between_orders|   avg_basket_size|      reorder_ratio|prod_times_ordered| prod_reorder_ratio|times_purchased|avg_add_to_cart_position|ever_reordered|\n",
            "+-------+----------+---------+------------+-----------------------+------------------+-------------------+------------------+-------------------+---------------+------------------------+--------------+\n",
            "|      9|     47167|        0|           4|                   22.0|              24.5| 0.4409090909090909|              2553| 0.6568742655699178|              1|                    21.0|             0|\n",
            "|     14|      4210|        0|          14|      21.23076923076923|15.785714285714286| 0.3710018012813044|             36968| 0.7800530188270937|              1|                    23.0|             0|\n",
            "|     16|     48171|        0|           7|     19.333333333333332|11.666666666666666|0.37584175084175087|              7261| 0.5132901804159207|              1|                    12.0|             0|\n",
            "|     19|      5209|        1|          10|      9.333333333333334|22.666666666666668| 0.4147986478542034|                17|0.35294117647058826|              2|                    10.5|             1|\n",
            "|     19|      5209|        0|          10|      9.333333333333334|22.666666666666668| 0.4147986478542034|                17|0.35294117647058826|              2|                    10.5|             1|\n",
            "+-------+----------+---------+------------+-----------------------+------------------+-------------------+------------------+-------------------+---------------+------------------------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dim_users = spark.read.parquet(f\"{silver}/dim_users\")\n",
        "fup       = spark.read.parquet(f\"{silver}/fact_user_product\")   # user×product history\n",
        "prod_f    = spark.read.parquet(f\"{gold}/product_features\")\n",
        "op        = spark.read.parquet(f\"{silver}/fact_order_products\")\n",
        "fo        = spark.read.parquet(f\"{silver}/fact_orders\")\n",
        "\n",
        "# Attach user_id to line items (for label joining)\n",
        "op_with_user = op.join(fo.select(\"order_id\",\"user_id\"), \"order_id\", \"left\")\n",
        "\n",
        "# Build a supervised dataset on observed pairs (user, product) with label = reordered\n",
        "dataset = (op_with_user\n",
        "  .join(dim_users.select(\"user_id\",\"total_orders\",\"avg_days_between_orders\",\"avg_basket_size\",\"reorder_ratio\"),\n",
        "        \"user_id\",\"left\")\n",
        "  .join(prod_f, \"product_id\", \"left\")\n",
        "  .join(fup.select(\"user_id\",\"product_id\",\"times_purchased\",\"avg_add_to_cart_position\",\"ever_reordered\"),\n",
        "        [\"user_id\",\"product_id\"], \"left\")\n",
        "  .fillna({\"times_purchased\":0, \"avg_add_to_cart_position\":0.0, \"ever_reordered\":0,\n",
        "           \"prod_times_ordered\":0, \"prod_reorder_ratio\":0.0})\n",
        "  .select(\n",
        "      \"user_id\",\"product_id\",\n",
        "      \"reordered\",  # label\n",
        "      \"total_orders\",\"avg_days_between_orders\",\"avg_basket_size\",\"reorder_ratio\",\n",
        "      \"prod_times_ordered\",\"prod_reorder_ratio\",\n",
        "      \"times_purchased\",\"avg_add_to_cart_position\",\"ever_reordered\"\n",
        "  )\n",
        ")\n",
        "\n",
        "dataset.cache()\n",
        "print(\"Rows:\", dataset.count())\n",
        "dataset.limit(5).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-08-25T00:44:43.8330389Z",
              "execution_start_time": "2025-08-25T00:42:44.5310427Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "632403ba-4adb-4bc1-8ae7-e5442a5f3cf5",
              "queued_time": "2025-08-25T00:42:44.5297417Z",
              "session_id": "5",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparksmall",
              "state": "finished",
              "statement_id": 4,
              "statement_ids": [
                4
              ]
            },
            "text/plain": [
              "StatementMeta(sparksmall, 5, 4, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.8973\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "features = [\n",
        "    \"total_orders\",\"avg_days_between_orders\",\"avg_basket_size\",\"reorder_ratio\",\n",
        "    \"prod_times_ordered\",\"prod_reorder_ratio\",\n",
        "    \"times_purchased\",\"avg_add_to_cart_position\",\"ever_reordered\"\n",
        "]\n",
        "\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "lr = LogisticRegression(labelCol=\"reordered\", featuresCol=\"features\", maxIter=50, regParam=0.01)\n",
        "\n",
        "train, test = dataset.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "pipe = Pipeline(stages=[assembler, lr]).fit(train)\n",
        "\n",
        "pred = pipe.transform(test)\n",
        "auc = BinaryClassificationEvaluator(labelCol=\"reordered\", metricName=\"areaUnderROC\").evaluate(pred)\n",
        "print(\"AUC:\", round(auc, 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-08-25T00:50:36.1806212Z",
              "execution_start_time": "2025-08-25T00:48:17.5850559Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "ac2068d5-76a9-47fb-a74a-34ac2d0cf3b3",
              "queued_time": "2025-08-25T00:48:17.583776Z",
              "session_id": "5",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparksmall",
              "state": "finished",
              "statement_id": 5,
              "statement_ids": [
                5
              ]
            },
            "text/plain": [
              "StatementMeta(sparksmall, 5, 5, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ gold.predictions_reorder written\n"
          ]
        }
      ],
      "source": [
        "scored_all = pipe.transform(dataset) \\\n",
        "    .select(\"user_id\",\"product_id\", F.col(\"probability\").alias(\"prob_vec\"), F.col(\"prediction\").alias(\"pred\"), F.col(\"reordered\").alias(\"label\"))\n",
        "\n",
        "# Extract P(reorder=1) from the probability vector\n",
        "get1 = F.udf(lambda v: float(v[1]), \"double\")\n",
        "scored_all = scored_all.withColumn(\"reorder_prob\", get1(\"prob_vec\")).drop(\"prob_vec\")\n",
        "\n",
        "(scored_all.repartition(8)\n",
        "    .write.mode(\"overwrite\")\n",
        "    .parquet(f\"{gold}/predictions_reorder\"))\n",
        "\n",
        "print(\"✅ gold.predictions_reorder written\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-08-25T00:58:15.7863873Z",
              "execution_start_time": "2025-08-25T00:58:15.5455786Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "a90ee334-963d-47f9-85f5-6ce67ec030ef",
              "queued_time": "2025-08-25T00:58:15.5442749Z",
              "session_id": "5",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparksmall",
              "state": "finished",
              "statement_id": 6,
              "statement_ids": [
                6
              ]
            },
            "text/plain": [
              "StatementMeta(sparksmall, 5, 6, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# from earlier\n",
        "pipe   # fitted PipelineModel\n",
        "train, test = dataset.randomSplit([0.8, 0.2], seed=42)\n",
        "from pyspark.sql import functions as F, Window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-08-25T00:59:04.929244Z",
              "execution_start_time": "2025-08-25T00:58:25.7753233Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "fce1fc54-4dcc-40cb-b8d7-99c7bbcffc77",
              "queued_time": "2025-08-25T00:58:25.7740502Z",
              "session_id": "5",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparksmall",
              "state": "finished",
              "statement_id": 7,
              "statement_ids": [
                7
              ]
            },
            "text/plain": [
              "StatementMeta(sparksmall, 5, 7, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test rows: 6766403\n",
            "+-------+----------+---------+----------+-------------------+\n",
            "|user_id|product_id|reordered|prediction|p1                 |\n",
            "+-------+----------+---------+----------+-------------------+\n",
            "|16     |48171     |0        |0.0       |0.02287248472478387|\n",
            "|35     |42625     |0        |1.0       |0.5694348750904394 |\n",
            "|38     |37119     |0        |1.0       |0.6341471237784557 |\n",
            "|40     |29926     |1        |1.0       |0.7084113605295783 |\n",
            "|54     |23734     |0        |1.0       |0.6585116697468143 |\n",
            "+-------+----------+---------+----------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pred_test = pipe.transform(test) \\\n",
        "    .select(\"user_id\",\"product_id\",\"reordered\",\"probability\",\"prediction\")\n",
        "\n",
        "# probability is a DenseVector [P(0), P(1)] — pull P(1)\n",
        "get1 = F.udf(lambda v: float(v[1]), \"double\")\n",
        "pred_test = pred_test.withColumn(\"p1\", get1(\"probability\")).drop(\"probability\")\n",
        "\n",
        "pred_test.cache(); print(\"test rows:\", pred_test.count())\n",
        "pred_test.show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-08-25T01:00:56.6658464Z",
              "execution_start_time": "2025-08-25T01:00:26.9502916Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "811bdf57-4adc-423c-ab37-8fd5b96228c7",
              "queued_time": "2025-08-25T01:00:26.9489205Z",
              "session_id": "5",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparksmall",
              "state": "finished",
              "statement_id": 8,
              "statement_ids": [
                8
              ]
            },
            "text/plain": [
              "StatementMeta(sparksmall, 5, 8, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "thr | precision | recall | f1\n",
            "0.1 |     0.782 |  1.000 | 0.878\n",
            "0.2 |     0.782 |  1.000 | 0.878\n",
            "0.3 |     0.782 |  1.000 | 0.878\n",
            "0.4 |     0.782 |  1.000 | 0.878\n",
            "0.5 |     0.783 |  0.997 | 0.877\n",
            "0.6 |     0.803 |  0.947 | 0.869\n",
            "0.7 |     0.871 |  0.745 | 0.803\n",
            "0.8 |     0.927 |  0.480 | 0.632\n",
            "0.9 |     0.955 |  0.255 | 0.403\n",
            "Best F1 threshold: 0.2 F1: 0.878\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def pr_at_threshold(df, thr):\n",
        "    pred = df.withColumn(\"yhat\", (F.col(\"p1\") >= F.lit(thr)).cast(\"int\"))\n",
        "    tp = pred.filter((F.col(\"yhat\")==1) & (F.col(\"reordered\")==1)).count()\n",
        "    fp = pred.filter((F.col(\"yhat\")==1) & (F.col(\"reordered\")==0)).count()\n",
        "    fn = pred.filter((F.col(\"yhat\")==0) & (F.col(\"reordered\")==1)).count()\n",
        "    precision = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
        "    recall    = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
        "    f1 = 2*precision*recall/(precision+recall) if (precision+recall)>0 else 0.0\n",
        "    return precision, recall, f1\n",
        "\n",
        "thresholds = [round(x,2) for x in np.linspace(0.1,0.9,9)]\n",
        "metrics = []\n",
        "for t in thresholds:\n",
        "    p,r,f1 = pr_at_threshold(pred_test, t)\n",
        "    metrics.append((t,p,r,f1))\n",
        "    \n",
        "print(\"thr | precision | recall | f1\")\n",
        "for t,p,r,f1 in metrics:\n",
        "    print(f\"{t:>3} | {p:9.3f} | {r:6.3f} | {f1:5.3f}\")\n",
        "\n",
        "# choose the best F1 threshold (or pick based on business need)\n",
        "best_thr, _, _, best_f1 = max(metrics, key=lambda x: x[3])\n",
        "print(\"Best F1 threshold:\", best_thr, \"F1:\", round(best_f1,3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-08-25T01:01:51.5699743Z",
              "execution_start_time": "2025-08-25T01:01:28.6940366Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "5b1231d2-10f5-4b62-a67c-7795486b8723",
              "queued_time": "2025-08-25T01:01:28.6928239Z",
              "session_id": "5",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparksmall",
              "state": "finished",
              "statement_id": 9,
              "statement_ids": [
                9
              ]
            },
            "text/plain": [
              "StatementMeta(sparksmall, 5, 9, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision@5: 0.6694\n",
            "Recall@5: 0.517\n"
          ]
        }
      ],
      "source": [
        "K = 5  # try 5, 10, 20\n",
        "\n",
        "# For each user, rank products by predicted probability\n",
        "w = Window.partitionBy(\"user_id\").orderBy(F.col(\"p1\").desc())\n",
        "ranked = pred_test.withColumn(\"rank\", F.row_number().over(w))\n",
        "\n",
        "topk = ranked.filter(F.col(\"rank\") <= K) \\\n",
        "             .select(\"user_id\",\"product_id\",\"reordered\",\"p1\",\"rank\")\n",
        "\n",
        "# Precision@K: of the K we recommended, what fraction were actually reordered?\n",
        "prec_k = (topk.groupBy(\"user_id\")\n",
        "              .agg(F.avg(F.col(\"reordered\").cast(\"double\")).alias(\"prec_at_k\"))\n",
        "              .agg(F.avg(\"prec_at_k\").alias(\"Precision_at_K\"))\n",
        "         ).collect()[0][\"Precision_at_K\"]\n",
        "\n",
        "# Recall@K: of the items the user actually reordered, how many did we capture in top-K?\n",
        "# First, number of positives per user in the test set:\n",
        "pos_per_user = (pred_test.groupBy(\"user_id\")\n",
        "                         .agg(F.sum(F.col(\"reordered\").cast(\"int\")).alias(\"pos\")))\n",
        "# True positives in top-K:\n",
        "tp_topk = (topk.groupBy(\"user_id\")\n",
        "                .agg(F.sum(F.col(\"reordered\").cast(\"int\")).alias(\"tp_in_topk\")))\n",
        "\n",
        "recall_k = (tp_topk.join(pos_per_user, \"user_id\", \"left\")\n",
        "                 .withColumn(\"rec_at_k\", F.when(F.col(\"pos\")>0, F.col(\"tp_in_topk\")/F.col(\"pos\")).otherwise(F.lit(None)))\n",
        "                 .agg(F.avg(\"rec_at_k\").alias(\"Recall_at_K\"))\n",
        "                 .collect()[0][\"Recall_at_K\"])\n",
        "\n",
        "print(f\"Precision@{K}:\", round(float(prec_k or 0.0), 4))\n",
        "print(f\"Recall@{K}:\", round(float(recall_k or 0.0), 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Average Precision for a single user@K: mean of precision@i at each hit position i<=K\n",
        "def avg_precision_at_k(pdf, K=5):\n",
        "    pdf = pdf.sort_values(\"p1\", ascending=False).head(K)\n",
        "    hits = 0\n",
        "    precisions = []\n",
        "    for i, (_, row) in enumerate(pdf.iterrows(), start=1):\n",
        "        if row[\"reordered\"] == 1:\n",
        "            hits += 1\n",
        "            precisions.append(hits / i)\n",
        "    return sum(precisions)/min(K, max(1, int(pdf[\"reordered\"].sum()))) if precisions else 0.0\n",
        "\n",
        "import pandas as pd\n",
        "mapk = (pred_test.select(\"user_id\",\"product_id\",\"reordered\",\"p1\")\n",
        "              .toPandas()\n",
        "              .groupby(\"user_id\")\n",
        "              .apply(lambda g: avg_precision_at_k(g, K=K))\n",
        "              .mean())\n",
        "\n",
        "print(f\"MAP@{K}:\", round(float(mapk), 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2025-08-25T01:06:01.0247372Z",
              "execution_start_time": "2025-08-25T01:05:38.1247611Z",
              "livy_statement_state": "available",
              "normalized_state": "finished",
              "parent_msg_id": "9a250ed6-d5f6-4ece-82d2-1e66491d1132",
              "queued_time": "2025-08-25T01:05:38.1235056Z",
              "session_id": "5",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": "sparksmall",
              "state": "finished",
              "statement_id": 11,
              "statement_ids": [
                11
              ]
            },
            "text/plain": [
              "StatementMeta(sparksmall, 5, 11, Finished, Available, Finished)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ gold.topn_recommendations written\n",
            "+----------+-------+------------------+----+-----------------------------------------------------+----------------------+---------------+\n",
            "|product_id|user_id|p1                |rank|product_name                                         |aisle_name            |department_name|\n",
            "+----------+-------+------------------+----+-----------------------------------------------------+----------------------+---------------+\n",
            "|196       |1      |0.8708614017612107|1   |Soda                                                 |soft drinks           |beverages      |\n",
            "|196       |1      |0.8708614017612107|2   |Soda                                                 |soft drinks           |beverages      |\n",
            "|196       |1      |0.8708614017612107|3   |Soda                                                 |soft drinks           |beverages      |\n",
            "|196       |1      |0.8708614017612107|4   |Soda                                                 |soft drinks           |beverages      |\n",
            "|196       |1      |0.8708614017612107|5   |Soda                                                 |soft drinks           |beverages      |\n",
            "|12427     |1      |0.8484055362673869|6   |Original Beef Jerky                                  |popcorn jerky         |snacks         |\n",
            "|10258     |1      |0.8446672979938525|7   |Pistachios                                           |nuts seeds dried fruit|snacks         |\n",
            "|10258     |1      |0.8446672979938525|8   |Pistachios                                           |nuts seeds dried fruit|snacks         |\n",
            "|10258     |1      |0.8446672979938525|9   |Pistachios                                           |nuts seeds dried fruit|snacks         |\n",
            "|10258     |1      |0.8446672979938525|10  |Pistachios                                           |nuts seeds dried fruit|snacks         |\n",
            "|32792     |2      |0.8126467556135547|1   |Chipotle Beef & Pork Realstick                       |popcorn jerky         |snacks         |\n",
            "|32792     |2      |0.8126467556135547|2   |Chipotle Beef & Pork Realstick                       |popcorn jerky         |snacks         |\n",
            "|24852     |2      |0.8087697430750737|3   |Banana                                               |fresh fruits          |produce        |\n",
            "|24852     |2      |0.8087697430750737|4   |Banana                                               |fresh fruits          |produce        |\n",
            "|47209     |2      |0.7891640917160645|5   |Organic Hass Avocado                                 |fresh fruits          |produce        |\n",
            "|1559      |2      |0.7387549991914433|6   |Cherry Pomegranate Greek Yogurt                      |yogurt                |dairy eggs     |\n",
            "|16589     |2      |0.7362232915519813|7   |Plantain Chips                                       |chips pretzels        |snacks         |\n",
            "|12000     |2      |0.7161513583582382|8   |Baked Organic Sea Salt Crunchy Pea Snack             |condiments            |pantry         |\n",
            "|33754     |2      |0.7048276701990696|9   |Total 2% with Strawberry Lowfat Greek Strained Yogurt|yogurt                |dairy eggs     |\n",
            "|32139     |2      |0.6926492502310895|10  |Hommus Classic Original                              |fresh dips tapenades  |deli           |\n",
            "+----------+-------+------------------+----+-----------------------------------------------------+----------------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "N = 10\n",
        "w = Window.partitionBy(\"user_id\").orderBy(F.col(\"p1\").desc())\n",
        "topn = pred_test.withColumn(\"rank\", F.row_number().over(w)) \\\n",
        "                .filter(F.col(\"rank\") <= N) \\\n",
        "                .select(\"user_id\",\"product_id\",\"p1\",\"rank\")\n",
        "\n",
        "# (Optional) attach product names\n",
        "account =  # your storage account name\n",
        "container =  # your container name\n",
        "prod = spark.read.parquet(f\"abfss://{container}@{account}.dfs.core.windows.net/silver/dim_products\") \\\n",
        "                 .select(\"product_id\",\"product_name\",\"aisle_name\",\"department_name\")\n",
        "\n",
        "topn_named = (topn.join(prod, \"product_id\", \"left\")\n",
        "                   .orderBy(\"user_id\",\"rank\"))\n",
        "\n",
        "(topn_named.repartition(8)\n",
        "    .write.mode(\"overwrite\")\n",
        "    .parquet(f\"abfss://{container}@{account}.dfs.core.windows.net/gold/topn_recommendations\"))\n",
        "\n",
        "print(\"✅ gold.topn_recommendations written\")\n",
        "topn_named.show(20, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
